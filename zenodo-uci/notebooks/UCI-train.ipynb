{
 "cells": [
  {
   "cell_type": "code",
   "id": "2c3ffa8c-2a42-4d03-920b-bf276ab2ae83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:57:19.684637Z",
     "start_time": "2024-11-06T13:57:19.672475Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:57:20.290612Z",
     "start_time": "2024-11-06T13:57:19.935982Z"
    }
   },
   "cell_type": "code",
   "source": "ls",
   "id": "23686cb9c19cd372",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_explanation_cache_checkpoint.pkl\r\n",
      "anchor_explanations_coverage.pkl\r\n",
      "auxiliary_mushrooms.py\r\n",
      "best_model_seq_contrastive.keras\r\n",
      "dice_explanation_cache_checkpoint.pkl\r\n",
      "dice_explanation_cache_checkpoint_test.pkl\r\n",
      "dice_explanations_coverage.pkl\r\n",
      "dummy_auc_palm_scale_100_window1.pkl\r\n",
      "dummy_auc_palm_scale_100_window2.pkl\r\n",
      "dummy_auc_palm_scale_100_window3.pkl\r\n",
      "dummy_auc_palm_scale_100_window4.pkl\r\n",
      "dummy_auc_palm_scale_100_window5.pkl\r\n",
      "dummy_auc_palm_scale_100_window6.pkl\r\n",
      "dummy_auc_palm_scale_100_window7.pkl\r\n",
      "dummy_prop_perturbation_scale_10_acc.pkl\r\n",
      "dummy_prop_perturbation_scale_10_acc_test_fin.pkl\r\n",
      "dummy_prop_perturbation_scale_10_acc_train_fin.pkl\r\n",
      "\u001B[0m\u001B[01;34mextracted_data\u001B[0m/\r\n",
      "\u001B[01;34mfast\u001B[0m/\r\n",
      "formatted_table_image.png\r\n",
      "lime_auc_palm_scale_100_w0.pkl\r\n",
      "lime_auc_palm_scale_100_window1.pkl\r\n",
      "lime_auc_palm_scale_100_window2.pkl\r\n",
      "lime_auc_palm_scale_100_window3.pkl\r\n",
      "lime_auc_palm_scale_100_window4.pkl\r\n",
      "lime_auc_palm_scale_100_window5.pkl\r\n",
      "lime_explanation_cache_checkpoint.pkl\r\n",
      "lime_explanations.pkl\r\n",
      "lime_explanations_v02.pkl_window1\r\n",
      "lime_explanations_v11.pkl\r\n",
      "lime_explanations_v12_train.pkl\r\n",
      "lime_explanations_v13_train.pkl\r\n",
      "lime_explanations_v13_train.pkl_window1\r\n",
      "lime_explanations_v13_train.pkl_window2\r\n",
      "lime_explanations_v13_train.pkl_window3\r\n",
      "lime_explanations_v13_train.pkl_window4\r\n",
      "lime_explanations_v13_train.pkl_window5\r\n",
      "lime_perturbational_acc_loss_100.pkl\r\n",
      "lime_perturbational_acc_loss_100_v2.pkl\r\n",
      "lime_perturbational_acc_loss.pkl\r\n",
      "lime_prop_perturbation_acc.pkl\r\n",
      "lime_prop_perturbation_scale_100_acc_test_fin.pkl\r\n",
      "lime_prop_perturbation_scale_100_acc_train_fin.pkl\r\n",
      "lime_prop_perturbation_scale_10_acc.pkl\r\n",
      "lime_prop_perturbation_scale_10_acc_test_fin.pkl\r\n",
      "lime_prop_perturbation_scale_10_acc_train_fin.pkl\r\n",
      "\u001B[01;34mmodels\u001B[0m/\r\n",
      "\u001B[01;34mMushroomDataset\u001B[0m/\r\n",
      "\u001B[01;34mmushroom-serialised\u001B[0m/\r\n",
      "\u001B[01;34m__pycache__\u001B[0m/\r\n",
      "\u001B[01;36mREADME.txt\u001B[0m@\r\n",
      "\u001B[01;34mresults\u001B[0m/\r\n",
      "shap_explanation_cache_checkpoint.pkl\r\n",
      "shap_perturbational_acc_loss_100.pkl\r\n",
      "shap_perturbational_acc_loss.pkl\r\n",
      "shap_prop_perturbation_acc.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc_test_fin.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc_test.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc_train_fin.pkl\r\n",
      "\u001B[01;34mshared\u001B[0m/\r\n",
      "\u001B[01;34mslow\u001B[0m/\r\n",
      "\u001B[01;34mutils\u001B[0m/\r\n",
      "\u001B[01;34mXAI-ISI\u001B[0m/\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "98ee37ad-06fd-4b3b-a2d5-f79467354f3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:57:59.484549Z",
     "start_time": "2024-11-06T13:57:55.638951Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 0 - first gpu, 1 - second, \"0,1\" - both gpu, first used\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import math\n",
    "import shap\n",
    "\n",
    "from aeon.datasets import load_classification\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Reshape, ConvLSTM1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras.initializers import glorot_uniform\n",
    "from tensorflow.python.util import deprecation\n",
    "from utils.windowshap import SlidingWindowSHAP\n",
    "\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from tensorflow.python.keras.layers import Dense, Flatten, Dropout, Reshape\n",
    "\n",
    "print(tf.__version__)  # print(tf.keras.__version__)\n",
    "\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "ops.logging.set_verbosity(ops.logging.ERROR)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "os.environ['TF_ENABLE_DEPRECATION_WARNINGS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(gpus)\n",
    "gpu_to_limit = gpus[0]  # or 1, watch out for CUDA_VISIBLE_DEVICES\n",
    "print(gpu_to_limit)\n",
    "\n",
    "gpu_mem_mb = 24564  # tyle mamy\n",
    "tf.config.set_logical_device_configuration(\n",
    "    gpu_to_limit, [tf.config.LogicalDeviceConfiguration(memory_limit=gpu_mem_mb // 4)]\n",
    ")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "data_path = \"shared/UCI-Benchmark/\"\n",
    "batch_size = 64"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 13:57:56.280919: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 13:57:57.158845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n",
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/uci_38/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "4e5897d9-89ca-457d-b6bd-4f4ed1ac878d",
   "metadata": {},
   "source": [
    "# Real Data example"
   ]
  },
  {
   "cell_type": "code",
   "id": "a94117ee-3b54-4f54-88ee-8b995b1979de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:58:01.892761Z",
     "start_time": "2024-11-06T13:58:01.836762Z"
    }
   },
   "source": [
    "# convlstm model\n",
    "def divisorGenerator(n):\n",
    "    large_divisors = []\n",
    "    for i in range(1, int(math.sqrt(n) + 1)):\n",
    "        if n % i == 0:\n",
    "            yield i\n",
    "            if i * i != n:\n",
    "                large_divisors.append(n / i)\n",
    "    for divisor in reversed(large_divisors):\n",
    "        yield divisor\n",
    "\n",
    "\n",
    "def load_dataset_aeon(dsname):\n",
    "    meta_data = None\n",
    "    if len(load_classification(dsname)) == 3:\n",
    "        X, y, meta_data = load_classification(dsname)\n",
    "    elif len(load_classification(dsname)) == 2:\n",
    "        print(\"Warning: no meta_data\")\n",
    "        X, y = load_classification(dsname)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset\")\n",
    "\n",
    "    X = np.moveaxis(X, 1, 2)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    # Print to check label transformation\n",
    "    print(\"Transformed labels:\", y[:10])\n",
    "    print(\"Unique classes after encoding:\", le.classes_)\n",
    "\n",
    "    trainX, testX, trainy, testy = train_test_split(X, y)\n",
    "\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "\n",
    "def generate_label_weights(train_y):\n",
    "    # Count the occurrences of each class by summing across rows\n",
    "    class_counts = np.sum(train_y, axis=0)\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = np.sum(class_counts)\n",
    "\n",
    "    # Calculate the weight for each class\n",
    "    class_weights = {}\n",
    "    num_classes = train_y.shape[1]\n",
    "    for i in range(num_classes):\n",
    "        class_weights[i] = total_samples / (num_classes * class_counts[i])\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def train_lstmconv_model(trainX, trainy, testX, testy, kernel=9):\n",
    "    # Clear previous session to avoid issues with variable initialization\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\n",
    "    dg = divisorGenerator(n_timesteps)\n",
    "    n_steps = int([next(dg) for i in range(3)][-1])\n",
    "    n_length = int(n_timesteps / n_steps)\n",
    "\n",
    "    print(f'Input shape (time, rows, channels): {(n_steps, n_length, n_features)}')\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((n_steps, n_length, n_features), input_shape=(n_timesteps, n_features)))\n",
    "    # model.add(Reshape((n_steps, n_length, n_features), input_shape=trainX.shape[1:]))\n",
    "    model.add(ConvLSTM1D(filters=64, kernel_size=kernel, padding='same', activation='relu', strides=1,\n",
    "                         data_format='channels_last',\n",
    "                         return_sequences=True,\n",
    "                         input_shape=(n_steps, n_length, n_features))\n",
    "              )\n",
    "    model.add(ConvLSTM1D(filters=32, kernel_size=kernel, padding='same', activation='relu', strides=1,\n",
    "                         data_format='channels_last',\n",
    "                         return_sequences=True,\n",
    "                         input_shape=(n_steps, n_length, n_features))\n",
    "              )\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten(name='embedding'))  #is this a latent representation we could use for clustering?\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Build the model explicitly before showing summary (not always necessary)\n",
    "    model.build((None, n_timesteps, n_features))\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit network\n",
    "    weights = generate_label_weights(trainy)\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, class_weight=weights)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy, model\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "\n",
    "# run an experiment\n",
    "def evaluate_lstmconv_model(dataset, repeats=1, trainX=None, trainy=None, testX=None, testy=None):\n",
    "    # load data\n",
    "    if trainX is None or trainy is None or testX is None or testy is None:\n",
    "        trainX, trainy, testX, testy = load_dataset_aeon(dataset)\n",
    "\n",
    "    #fill missing with zeros\n",
    "    trainX = np.nan_to_num(trainX, nan=0)\n",
    "    testX = np.nan_to_num(testX, nan=0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    trainX = scaler.fit_transform(trainX.reshape(-1, trainX.shape[-1])).reshape(trainX.shape)\n",
    "    testX = scaler.transform(testX.reshape(-1, testX.shape[-1])).reshape(testX.shape)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score, model = train_lstmconv_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r + 1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return model, trainX, trainy, testX, testy, score\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d050ffed-4f3a-4454-bd51-76c56d27626a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:58:03.004826Z",
     "start_time": "2024-11-06T13:58:02.942738Z"
    }
   },
   "source": [
    "def save_bundle(model, trainX, trainy, testX, testy, svtr, svts, dsname,\n",
    "                dir='./results', explain_prefix=\"sv\", only_explain=False):\n",
    "    if not os.path.isdir(f'{dir}/{dsname}'):\n",
    "        os.makedirs(f'{dir}/{dsname}')\n",
    "\n",
    "    if not only_explain:\n",
    "        # model.save(f'{dir}/{dsname}/model.h5', save_format='h5')\n",
    "        model.save(f'{dir}/{dsname}/model_tf', save_format='tf')\n",
    "\n",
    "        pickle.dump(trainX, open(f'{dir}/{dsname}/trainX.pickle', 'wb'))\n",
    "        pickle.dump(trainy, open(f'{dir}/{dsname}/trainy.pickle', 'wb'))\n",
    "        pickle.dump(testX, open(f'{dir}/{dsname}/testX.pickle', 'wb'))\n",
    "        pickle.dump(testy, open(f'{dir}/{dsname}/testy.pickle', 'wb'))\n",
    "\n",
    "    pickle.dump(svtr, open(f'{dir}/{dsname}/{explain_prefix}tr.pickle', 'wb'))\n",
    "    pickle.dump(svts, open(f'{dir}/{dsname}/{explain_prefix}ts.pickle', 'wb'))\n",
    "\n",
    "\n",
    "def load_bundle(dsname, dir='./results', explain_prefix=\"sv\", only_explain=False):\n",
    "    model, trainX, trainy, testX, testy = None, None, None, None, None,\n",
    "    if not only_explain:\n",
    "        tf_model_path = f'{dir}/{dsname}/model_tf'\n",
    "        h5_model_path = f'{dir}/{dsname}/model.h5'\n",
    "\n",
    "        if os.path.exists(tf_model_path):\n",
    "            print(\"Loading model in tf format...\")\n",
    "            model = tf.keras.models.load_model(tf_model_path)\n",
    "        elif os.path.exists(h5_model_path):\n",
    "            print(\"Loading model in h5 format...\")\n",
    "            model = tf.keras.models.load_model(h5_model_path,\n",
    "                                               custom_objects={'GlorotUniform': glorot_uniform,\n",
    "                                                               'ConvLSTM1D': ConvLSTM1D})\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f'Nie znaleziono modelu w formatach SavedModel ani H5 dla {dsname} w katalogu {dir}.')\n",
    "        # print(model.summary())\n",
    "\n",
    "        trainX = pickle.load(open(f'{dir}/{dsname}/trainX.pickle', 'rb'))\n",
    "        trainy = pickle.load(open(f'{dir}/{dsname}/trainy.pickle', 'rb'))\n",
    "        testX = pickle.load(open(f'{dir}/{dsname}/testX.pickle', 'rb'))\n",
    "        testy = pickle.load(open(f'{dir}/{dsname}/testy.pickle', 'rb'))\n",
    "\n",
    "    svtr = pickle.load(open(f'{dir}/{dsname}/{explain_prefix}tr.pickle', 'rb'))\n",
    "    svts = pickle.load(open(f'{dir}/{dsname}/{explain_prefix}ts.pickle', 'rb'))\n",
    "\n",
    "    return model, trainX, trainy, testX, testy, svtr, svts"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and test multivariate/EthanolConcentration",
   "id": "d21d5afe48721a67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T20:47:30.838387Z",
     "start_time": "2024-11-04T20:47:30.784163Z"
    }
   },
   "cell_type": "code",
   "source": "model_to_fix = \"EthanolConcentration\"",
   "id": "fae92a884ee6b891",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:44:56.448716Z",
     "start_time": "2024-11-04T18:44:55.219647Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 18:44:55.690744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6141 MB memory:  -> device: 0, name: NVIDIA RTX A5500, pci bus id: 0000:9c:00.0, compute capability: 8.6\n",
      "2024-11-04 18:44:55.703633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-11-04 18:44:55.820451: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_1/bias/Assign' id:688 op device:{requested: '', assigned: ''} def:{{{node dense_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/bias, dense_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-11-04 18:44:56.076527: W tensorflow/c/c_api.cc:304] Operation '{name:'conv_lstm1d_1/bias/v/Assign' id:933 op device:{requested: '', assigned: ''} def:{{{node conv_lstm1d_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv_lstm1d_1/bias/v, conv_lstm1d_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x7f0a06911af0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6,
   "source": [
    "model, trainXo, trainyo, testX, testy, trainsvo, testsv = load_bundle(model_to_fix, dir=data_path + 'ds/multivariate')\n",
    "\n",
    "model"
   ],
   "id": "86f5b7a19e9ab6ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:17:49.728410Z",
     "start_time": "2024-11-04T19:17:49.691446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_timesteps, n_features = trainXo.shape[1], trainXo.shape[2]\n",
    "print(n_timesteps, n_features)\n",
    "\n",
    "n_timesteps, n_features = testX.shape[1], testX.shape[2]\n",
    "print(n_timesteps, n_features)"
   ],
   "id": "e8210a50bee3177a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1751 3\n",
      "1751 3\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:17:53.362791Z",
     "start_time": "2024-11-04T19:17:51.972613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainX_reshaped = trainXo.reshape(-1, n_timesteps * n_features)\n",
    "_index = -1\n",
    "prediction = model.predict(trainX_reshaped[_index].reshape(-1, n_timesteps, n_features))\n",
    "prediction"
   ],
   "id": "e473e4b67bd1bb98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 19:17:52.029082: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_1/Softmax' id:694 op device:{requested: '', assigned: ''} def:{{{node dense_1/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-11-04 19:17:52.168667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-11-04 19:17:52.277080: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-04 19:17:52.278549: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-04 19:17:52.278585: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-11-04 19:17:52.279593: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-04 19:17:52.279647: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-11-04 19:17:53.326290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:17:55.332982Z",
     "start_time": "2024-11-04T19:17:55.221733Z"
    }
   },
   "cell_type": "code",
   "source": "trainsvo[0]",
   "id": "1e5ed7b1228d2b99",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:32:54.287119Z",
     "start_time": "2024-11-04T19:32:54.243418Z"
    }
   },
   "cell_type": "code",
   "source": "trainX_reshaped[:2, :10]",
   "id": "d47e32f7b8f2becf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.75419424, -0.76078121, -0.76012238, -0.75452964, -0.76086456,\n",
       "        -0.75996785, -0.75480755, -0.76081826, -0.7601042 , -0.75480755],\n",
       "       [-0.91855807, -0.91569179, -0.91378102, -0.91907277, -0.91590387,\n",
       "        -0.91372466, -0.91888545, -0.91611195, -0.91368321, -0.91835152]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:56:15.673276Z",
     "start_time": "2024-11-04T19:56:14.056228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "type(load_classification(model_to_fix))\n",
    "len(load_classification(model_to_fix))\n",
    "X, y = load_classification(model_to_fix)\n",
    "X.shape"
   ],
   "id": "9ecae9dc833a52b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(524, 3, 1751)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:56:04.138676Z",
     "start_time": "2024-11-04T19:56:03.930516Z"
    }
   },
   "cell_type": "code",
   "source": "y.size",
   "id": "41b95f23260a03d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T20:11:02.625309Z",
     "start_time": "2024-11-04T20:11:02.385517Z"
    }
   },
   "cell_type": "code",
   "source": "y[:10]",
   "id": "b8e25a5701072a7f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['e35', 'e35', 'e35', 'e35', 'e35', 'e35', 'e35', 'e35', 'e35',\n",
       "       'e35'], dtype='<U3')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "de3292f7-9906-4d12-b6a2-5b6423420271",
   "metadata": {},
   "source": "# Multivariate model training"
  },
  {
   "cell_type": "code",
   "id": "896f742a-6517-475c-99c2-126a46e442ba",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-04T20:47:37.979570Z"
    }
   },
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# df=pd.read_csv('./datasets/summaryMultivariate.csv',header=None,sep='|')\n",
    "# df = df[0].str.split(',', expand=True).dropna(axis=1)\n",
    "# df.columns = df.iloc[0]\n",
    "# df = df[1:]\n",
    "\n",
    "bg_size = 1000\n",
    "window_len = 23\n",
    "stride = 10\n",
    "shap_version = 'deep'  #'window'\n",
    "absshap = True  #Wheather to use absolut values for shap, or use sign of shapp in changepoint detection\n",
    "#limits due to memory limitations of GPUs\n",
    "SEQUENCE_LENGTH_LIMIT = 2000\n",
    "DIMENSIONALITY_LIMIT = 300\n",
    "ACC_LIMIT = 0\n",
    "\n",
    "directory = 'multivariate'\n",
    "\n",
    "# skipds = ['CharacterTrajectories',\n",
    "#  'InsectWingbeat',\n",
    "#  'JapaneseVowels',\n",
    "#  'SpokenArabicDigits',\n",
    "#  'AtrialFibrillation',\n",
    "#  'Epilepsy',\n",
    "#  'FaceDetection',\n",
    "#  'FingerMovements',\n",
    "#  'HandMovementDirection',\n",
    "#  'Handwriting',\n",
    "#  'Libras',\n",
    "#  'LSST',\n",
    "#  'PenDigits']\n",
    "\n",
    "skipds = []\n",
    "summary = []\n",
    "\n",
    "for p in [model_to_fix]:  # df['Problem']\n",
    "    print(f'Time for {p}')\n",
    "    if p in skipds:\n",
    "        print(f'Skipping: {p}')\n",
    "        continue\n",
    "    # try:\n",
    "    X, y = load_classification(p)\n",
    "    X = np.moveaxis(X, 1, 2)\n",
    "    if X.shape[1] > SEQUENCE_LENGTH_LIMIT:\n",
    "        print(f'Skipping: {p} due to sequence length')\n",
    "        summary.append([p, np.nan, 'LENGTH'])\n",
    "        continue\n",
    "    if X.shape[2] > DIMENSIONALITY_LIMIT:\n",
    "        print(f'Skipping: {p} due to dimensionality size')\n",
    "        summary.append([p, np.nan, 'DIMENSIONALITY'])\n",
    "        continue\n",
    "\n",
    "    print(f'Classifying: {p}')\n",
    "\n",
    "    # _, trainX, trainy, testX, testy, _, _ = load_bundle(p, dir=os.path.join(data_path, 'ds', directory))\n",
    "\n",
    "    model, trainX, trainy, testX, testy, score = evaluate_lstmconv_model(p)\n",
    "    if score < ACC_LIMIT:\n",
    "        print(f'Skipping: {p} due to low score which is {score}')\n",
    "        skipds.append(p)\n",
    "        continue\n",
    "    print(f'Size of the problem: {trainX.shape}')\n",
    "\n",
    "    if shap_version == 'window':\n",
    "        #There is a problem with Windowed version for more than two classes\n",
    "        indexes = np.arange(0, len(trainX))\n",
    "        np.random.shuffle(indexes)\n",
    "        maxid = min(bg_size, len(trainX))\n",
    "        background_data = trainX[indexes[:maxid]]\n",
    "\n",
    "        sv_ts = np.zeros((len(testX), testX.shape[1], testX.shape[2]))\n",
    "        sv_tr = np.zeros((len(trainX), trainX.shape[1], trainX.shape[2]))\n",
    "\n",
    "        for i in range(len(testX)):\n",
    "            gtw = SlidingWindowSHAP(model, stride, window_len, background_data, testX[i:i + 1], model_type='lstm')\n",
    "            sv_ts[i, :, :] = gtw.shap_values(num_output=trainy.shape[1])\n",
    "        for i in range(len(trainX)):\n",
    "            gtw = SlidingWindowSHAP(model, stride, window_len, background_data, trainX[i:i + 1], model_type='lstm')\n",
    "            sv_tr[i, :, :] = gtw.shap_values(num_output=trainy.shape[1])\n",
    "    elif shap_version == 'deep':\n",
    "        indexes = np.arange(0, len(trainX))\n",
    "        np.random.shuffle(indexes)\n",
    "        maxid = min(bg_size, len(trainX))\n",
    "        background_data = trainX[indexes[:maxid]]\n",
    "\n",
    "        explainer = shap.DeepExplainer(model, background_data)\n",
    "        shap_values_ts = explainer.shap_values(testX, check_additivity=False)\n",
    "        shap_values_tr = explainer.shap_values(trainX, check_additivity=False)\n",
    "        if absshap:\n",
    "            sv_ts = abs(np.array(shap_values_ts)).mean(\n",
    "                axis=0)  # This basically returns the average importance over the feature/sample\n",
    "            # Not taking into account the sign of shap value, as it is not required\n",
    "            # for breakpoints calculation\n",
    "            sv_tr = abs(np.array(shap_values_tr)).mean(\n",
    "                axis=0)  # This basically returns the average importance over the feature/sample\n",
    "            # Not taking into account the sign of shap value, as it is not required\n",
    "            # for breakpoints calculation\n",
    "        else:\n",
    "            indexer = np.argmax(model.predict(testX), axis=1)\n",
    "            sv_ts = []\n",
    "            for i in range(0, len(testX)):\n",
    "                sv_ts.append([shap_values_ts[indexer[i]][i, :]])\n",
    "            sv_ts = np.concatenate(sv_ts)\n",
    "            indexer = np.argmax(model.predict(trainX), axis=1)\n",
    "            sv_tr = []\n",
    "            for i in range(0, len(trainX)):\n",
    "                sv_tr.append([shap_values_tr[indexer[i]][i, :]])\n",
    "            sv_tr = np.concatenate(sv_tr)\n",
    "\n",
    "    save_bundle(model, trainX, trainy, testX, testy, sv_tr, sv_ts, p, dir=os.path.join(data_path, 'ds', directory))\n",
    "    summary.append([p, score, 'OK'])\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     summary.append([p, np.nan, str(e)])\n",
    "    #     print(f'Failed...')\n",
    "    #     skipds.append(p)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# multi_model_summary = './results/multivariate-new/summary.csv'\n",
    "# pd.DataFrame(summary,columns=['dataset','score','status']).to_csv(multi_model_summary,index=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for EthanolConcentration\n",
      "Classifying: EthanolConcentration\n",
      "Warning: no meta_data\n",
      "Transformed labels: [0 0 0 0 0 0 0 0 0 0]\n",
      "Unique classes after encoding: ['e35' 'e38' 'e40' 'e45']\n",
      "Input shape (time, rows, channels): (103, 17, 3)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 103, 17, 3)        0         \n",
      "                                                                 \n",
      " conv_lstm1d (ConvLSTM1D)    (None, 103, 17, 64)       154624    \n",
      "                                                                 \n",
      " conv_lstm1d_1 (ConvLSTM1D)  (None, 103, 17, 32)       110720    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 103, 17, 32)       0         \n",
      "                                                                 \n",
      " embedding (Flatten)         (None, 56032)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               5603300   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5869048 (22.39 MB)\n",
      "Trainable params: 5869048 (22.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 20:47:41.006624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12282 MB memory:  -> device: 0, name: NVIDIA RTX A5500, pci bus id: 0000:51:00.0, compute capability: 8.6\n",
      "2024-11-04 20:47:41.037249: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-11-04 20:47:41.098073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 11.99GiB (12878610432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2024-11-04 20:47:41.246021: W tensorflow/c/c_api.cc:304] Operation '{name:'training/Adam/conv_lstm1d_1/kernel/v/Assign' id:2704 op device:{requested: '', assigned: ''} def:{{{node training/Adam/conv_lstm1d_1/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/conv_lstm1d_1/kernel/v, training/Adam/conv_lstm1d_1/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-11-04 20:47:41.905148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-11-04 20:47:41.967862: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-04 20:47:41.969049: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-04 20:47:41.969086: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-11-04 20:47:41.970544: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-04 20:47:41.970600: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-11-04 20:47:42.189145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 19.084\n",
      "[19.083969295024872]\n",
      "Accuracy: 19.084% (+/-0.000)\n",
      "Size of the problem: (393, 1751, 3)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af14de0e-c2db-4b7c-975f-05874e4f40b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T21:17:07.909116Z",
     "start_time": "2024-11-04T21:17:07.863608Z"
    }
   },
   "source": "summary",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EthanolConcentration', 19.083969295024872, 'OK']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T20:19:16.070996Z",
     "start_time": "2024-11-04T20:19:16.007728Z"
    }
   },
   "cell_type": "code",
   "source": "tf.compat.v1.initialize_all_variables()",
   "id": "6102bb6ca614ff6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.util.tf_should_use.ShouldUseWrapper object at 0x7f0934190580>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:35:35.401775Z",
     "start_time": "2024-11-05T08:35:34.034226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_new, trainX_new, trainy_new, testX_new, testy_new, trainsv_new, testsv_new = load_bundle(model_to_fix,\n",
    "                                                                                               dir=data_path + 'ds/multivariate')\n",
    "\n",
    "model_new"
   ],
   "id": "461b9fbf11096518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in tf format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 08:35:35.123755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12282 MB memory:  -> device: 0, name: NVIDIA RTX A5500, pci bus id: 0000:51:00.0, compute capability: 8.6\n",
      "2024-11-05 08:35:35.198756: W tensorflow/c/c_api.cc:304] Operation '{name:'AssignVariableOp_36' id:329 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node AssignVariableOp_36}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false, _device=\"/device:CPU:0\"](conv_lstm1d_1/bias/v, Identity_36)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x7f92b2cbeeb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:44:57.844391Z",
     "start_time": "2024-11-05T08:44:57.796052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_timesteps_new, n_features_new = trainX_new.shape[1], trainX_new.shape[2]\n",
    "print(n_timesteps_new, n_features_new)\n",
    "\n",
    "n_timesteps_new, n_features_new = testX_new.shape[1], testX_new.shape[2]\n",
    "print(n_timesteps_new, n_features_new)\n",
    "\n",
    "trainX_reshaped_new = trainX_new.reshape(-1, n_timesteps_new * n_features_new)\n",
    "trainX_reshaped_new[:2, :10]"
   ],
   "id": "b86605f380103be8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1751 3\n",
      "1751 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.91144945, -0.91760858, -0.91162117, -0.9119377 , -0.91810254,\n",
       "        -0.91160955, -0.91198368, -0.91808251, -0.91130938, -0.91170224],\n",
       "       [-0.74786201, -0.75629294, -0.75323544, -0.74813022, -0.75630243,\n",
       "        -0.75365401, -0.74815896, -0.75629294, -0.75386795, -0.74784285]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T08:48:48.966018Z",
     "start_time": "2024-11-05T08:48:48.785957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_index = -1\n",
    "prediction_new = model_new.predict(trainX_reshaped_new[_index].reshape(-1, n_timesteps_new, n_features_new))\n",
    "prediction_new"
   ],
   "id": "6fa84cf8d9035bab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check LIME explanations",
   "id": "3a5873688bf3bcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:58:51.204594Z",
     "start_time": "2024-11-06T13:58:50.388424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_multi, trainX_multi, trainy_multi, testX_multi, testy_multi, trainlv_multi, testlv_multi = load_bundle(\n",
    "    \"SelfRegulationSCP1\", dir=data_path + 'ds/multivariate', explain_prefix=\"lv\")\n",
    "model_uni, trainX_uni, trainy_uni, testX_uni, testy_uni, trainlv_uni, testlv_uni = load_bundle(\n",
    "    \"InsectWingbeatSound\", dir=data_path + 'ds/univariate', explain_prefix=\"lv\")"
   ],
   "id": "222aba7e0d08d62d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in h5 format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 13:58:50.733399: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2/bias/Assign' id:1709 op device:{requested: '', assigned: ''} def:{{{node dense_2/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/bias, dense_2/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-11-06 13:58:50.971119: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_1_1/bias/v/Assign' id:2001 op device:{requested: '', assigned: ''} def:{{{node dense_1_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1_1/bias/v, dense_1_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:02:53.597314Z",
     "start_time": "2024-11-06T14:02:53.536187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X[train]\", trainX_multi.shape, \"X[test]\", testX_multi.shape, \"y[train]\", trainy_multi.shape, \"y[test]\",\n",
    "      testy_multi.shape, \"LIME[train]\", trainlv_multi.shape, \"LIME[test]\", testlv_multi.shape)"
   ],
   "id": "41deccc55ed6a7bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[train] (420, 896, 6) X[test] (141, 896, 6) y[train] (420, 2) y[test] (141, 2) LIME[train] (420, 896, 6) LIME[test] (141, 896, 6)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:03:36.626307Z",
     "start_time": "2024-11-06T14:03:36.580540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X[train]\", trainX_uni.shape, \"X[test]\", testX_uni.shape, \"y[train]\", trainy_uni.shape, \"y[test]\",\n",
    "      testy_uni.shape, \"LIME[train]\", trainlv_uni.shape, \"LIME[test]\", testlv_uni.shape)"
   ],
   "id": "4cdeeed3425272cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[train] (1650, 256, 1) X[test] (550, 256, 1) y[train] (1650, 11) y[test] (550, 11) LIME[train] (1650, 256, 1) LIME[test] (550, 256, 1)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db7c0c4b19553bcc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
